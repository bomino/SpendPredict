This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-04T17:20:32.202Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
app.py
components/metrics.py
config/styles.py
model_management.py
model.py
models/model_metadata.json
preprocessing.py
requirements.txt

================================================================
Repository Files
================================================================

================
File: app.py
================
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import base64
from preprocessing import display_df_info, preprocess_data
from model import train_model, add_features
from model_management import ModelManager
from config.styles import get_css, apply_style,ColorPalette


def display_detailed_metrics(model, metrics, y_test, y_pred, target_col):
    col1, col2, col3, col4 = st.columns(4)
    colors = ColorPalette()
    
    metrics_data = [
        (col1, 'RMSE', metrics['rmse'], colors.primary),
        (col2, 'MAE', metrics['mae'], colors.success),
        (col3, 'RÂ² Score', metrics['r2'], colors.warning),
        (col4, 'CV Score', metrics['cv_scores'].mean(), metrics['cv_scores'].std(), colors.error)
    ]
    
    for col, title, *values, color in metrics_data:
        with col:
            value = f"{values[0]:.3f} Â± {values[1]:.3f}" if len(values) > 1 else f"{values[0]:.3f}"
            st.markdown(apply_style("metric",
                value=value,
                label=title
            ), unsafe_allow_html=True)

def create_download_link(df):
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    return f"""
        <div class="card">
            <a href="data:file/csv;base64,{b64}" 
               download="predictions.csv" 
               class="stButton">
                <button>Download Predictions CSV</button>
            </a>
        </div>
    """

def display_error_analysis(y_test, y_pred, metrics):
    error_df = pd.DataFrame({
        'Actual': y_test,
        'Predicted': y_pred,
        'Absolute_Error': np.abs(y_test - y_pred),
        'Percent_Error': metrics['percent_errors']
    })
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(apply_style("metric",
            value=f"{metrics['percent_errors'].mean():.1f}%",
            label="Average Error %"
        ), unsafe_allow_html=True)
    
    with col2:
        st.markdown(apply_style("metric",
            value=f"{np.percentile(metrics['percent_errors'], 90):.1f}%",
            label="90th Percentile Error"
        ), unsafe_allow_html=True)
    
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st.markdown("### Largest Prediction Errors")
    error_display = error_df.nlargest(10, 'Percent_Error')
    formatted_errors = pd.DataFrame({
        'Actual': error_display['Actual'].round(2),
        'Predicted': error_display['Predicted'].round(2),
        'Absolute_Error': error_display['Absolute_Error'].round(2),
        'Percent_Error': error_display['Percent_Error'].round(1).astype(str) + '%'
    })
    st.dataframe(formatted_errors, use_container_width=True)
    st.markdown('</div>', unsafe_allow_html=True)

def display_model_details(selected_metadata):
    st.subheader("Model Details")
    timestamp = selected_metadata['timestamp']
    formatted_date = pd.todatetime(timestamp, format='%Y%m%d%H%M%S').strftime('%B %d, %Y at %I:%M %p')

    st.markdown(apply_style("metric",
        value=selected_metadata['type'],
        label="Model Type"
    ), unsafe_allow_html=True)
    st.markdown(apply_style("metric",
        value=formatted_date,
        label="Trained on"
    ), unsafe_allow_html=True)

def display_visualizations(model, X_test, y_test, y_pred, target_col):
    col1, col2 = st.columns(2)
    with col1:
        residuals = y_test - y_pred
        fig_residuals = px.histogram(
            residuals,
            title="Distribution of Prediction Errors",
            labels={'value': 'Error', 'count': 'Frequency'},
            template='plotly_white'
        )
        fig_residuals.update_layout(
            showlegend=False,
            title_x=0.5,
            title_font_size=20,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig_residuals, use_container_width=True)
    
    with col2:
        fig_scatter = px.scatter(
            x=y_test, y=y_pred,
            labels={'x': f'Actual {target_col}', 'y': f'Predicted {target_col}'},
            title="Actual vs Predicted Values"
        )
        fig_scatter.add_trace(go.Scatter(
            x=[y_test.min(), y_test.max()],
            y=[y_test.min(), y_test.max()],
            mode='lines',
            name='Perfect Prediction',
            line=dict(color='red', dash='dash')
        ))
        fig_scatter.update_layout(
            template='plotly_white',
            title_x=0.5,
            title_font_size=20,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig_scatter, use_container_width=True)
    
    if hasattr(model[-1], 'feature_importances_'):
        feature_names = model[0].get_feature_names_out()
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': model[-1].feature_importances_
        }).sort_values('Importance', ascending=True)
        
        fig_importance = px.bar(
            importance_df.tail(10),
            x='Importance',
            y='Feature',
            orientation='h',
            title='Top 10 Most Important Features'
        )
        fig_importance.update_layout(
            template='plotly_white',
            title_x=0.5,
            title_font_size=20,
            height=400,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig_importance, use_container_width=True)



def display_all_tabs(model, metrics, X_test, y_test, y_pred, target_col):
    tab1, tab2, tab3 = st.tabs([
        "ðŸ“Š Key Metrics", 
        "ðŸ“ˆ Visualizations", 
        "ðŸŽ¯ Error Analysis"
    ])
    
    with tab1:
        display_detailed_metrics(model, metrics, y_test, y_pred, target_col)
    with tab2:
        display_visualizations(model, X_test, y_test, y_pred, target_col)
    with tab3:
        display_error_analysis(y_test, y_pred, metrics)


def main():
   st.set_page_config(
       page_title="ML Prediction Platform",
       page_icon="ðŸ¤–",
       layout="wide",
       initial_sidebar_state="expanded"
   )
   st.markdown(get_css(), unsafe_allow_html=True)
   colors = ColorPalette()

   # Initialize session states
   if 'model_manager' not in st.session_state:
       st.session_state.model_manager = ModelManager()
   for state in ['model', 'model_config', 'was_log_transformed', 'previous_tab']:
       if state not in st.session_state:
           st.session_state[state] = None

   # Header
   st.markdown("""
       <div class="app-header">
           <h1>ðŸ¤– ML Prediction Platform</h1>
           <p>Build, train, and manage your machine learning models</p>
       </div>
   """, unsafe_allow_html=True)

   # Model count metric
   st.markdown(apply_style("metric",
       value=len(st.session_state.model_manager.metadata),
       label="Active Models"
   ), unsafe_allow_html=True)

   # Main tabs
   tab1, tab2 = st.tabs(["ðŸ“Š Model Management", "ðŸ”§ Train New Model"])
   current_tab = "Model Management" if tab1._active else "Train New Model"
   
   if current_tab == "Model Management" and st.session_state.previous_tab != current_tab:
       st.session_state.model_manager = ModelManager()
   st.session_state.previous_tab = current_tab

   # Model Management Tab
   with tab1:
       if st.session_state.model_manager.metadata:
           st.markdown('<div class="card">', unsafe_allow_html=True)
           st.session_state.model_manager.display_comparison()
           st.markdown('</div>', unsafe_allow_html=True)
           
           st.markdown('<div class="card">', unsafe_allow_html=True)
           st.header("Select Model for Predictions")
           model_options = {f"{k} ({v['type']} - {v['timestamp']})": k 
                       for k, v in st.session_state.model_manager.metadata.items()}
           selected_model_name = st.selectbox("Select Model", list(model_options.keys()))
           selected_model_id = model_options[selected_model_name]
           
           if selected_model_id:
               selected_model = st.session_state.model_manager.get_model(selected_model_id)
               selected_metadata = st.session_state.model_manager.metadata[selected_model_id]
               
               col1, col2 = st.columns(2)
               with col1:
                   st.subheader("Model Details")
                   st.markdown(apply_style("metric",
                       value=selected_metadata['type'],
                       label="Type"
                   ), unsafe_allow_html=True)
                   st.markdown(apply_style("metric",
                       value=selected_metadata['timestamp'],
                       label="Trained on"
                   ), unsafe_allow_html=True)
               
               with col2:
                   st.subheader("Model Performance")
                   metrics_df = pd.DataFrame([{
                       'RÂ² Score': selected_metadata['metrics']['r2'],
                       'RMSE': selected_metadata['metrics']['rmse'],
                       'MAE': selected_metadata['metrics']['mae'],
                       'CV Score': selected_metadata['metrics']['cv_score']
                   }])
                   st.dataframe(metrics_df.round(4), use_container_width=True)
               
               # Model performance tabs
               if selected_metadata.get('test_data'):
                   X_test, y_test, y_pred = selected_metadata['test_data']
                   display_all_tabs(
                       selected_model,
                       selected_metadata['metrics'],
                       X_test, y_test, y_pred,
                       selected_metadata['config']['target_col']
                   )
               
               # Predictions section
               st.markdown('<div class="card">', unsafe_allow_html=True)
               st.header("Make Predictions")
               new_file = st.file_uploader("Upload new data", type="csv", key="new_data")
               
               if new_file:
                   try:
                       # Process new data
                       new_df = pd.read_csv(new_file)
                       config = st.session_state.model_manager.get_model_config(selected_model_id)
                       new_df_processed, _ = preprocess_data(
                           new_df,
                           config['date_col'],
                           config['cat_cols'],
                           config['target_col']
                       )
                       
                       new_df_processed, _ = add_features(
                           new_df_processed,
                           config['date_col'],
                           config['cat_cols'],
                           config['target_col']
                       )
                       
                       # Make predictions
                       predictions = selected_model.predict(
                           new_df_processed[config['required_columns']]
                       )
                       
                       # Handle log transformation
                       if st.session_state.was_log_transformed:
                           predictions = np.expm1(predictions)
                           actual_values = np.expm1(new_df_processed[config['target_col']])
                       else:
                           actual_values = new_df_processed[config['target_col']]
                       
                       # Display results
                       results_df = pd.DataFrame({
                           'Date': new_df_processed[config['date_col']],
                           'FinalCategory': new_df_processed[config['cat_cols'][0]],
                           'Actual_Spend': actual_values,
                           'Predicted_Spend': predictions,
                           'Difference': actual_values - predictions,
                           'Percent_Error': abs((actual_values - predictions) / actual_values) * 100
                       })
                       
                       st.markdown('<div class="card">', unsafe_allow_html=True)
                       st.dataframe(results_df.round(2), use_container_width=True)
                       st.markdown(create_download_link(results_df), unsafe_allow_html=True)
                       st.markdown('</div>', unsafe_allow_html=True)
                       
                   except Exception as e:
                       st.error(f"Error making predictions: {str(e)}")
               st.markdown('</div>', unsafe_allow_html=True)
           st.markdown('</div>', unsafe_allow_html=True)
       else:
           st.markdown(apply_style("status",
               status="warning",
               text="No trained models available. Switch to 'Train New Model' tab to train one."
           ), unsafe_allow_html=True)

   # Training Tab
   with tab2:
        st.markdown('<div class="card">', unsafe_allow_html=True)
        uploaded_file = st.file_uploader("Upload Training Data", type="csv")
        
        if uploaded_file:
            df = pd.read_csv(uploaded_file)
            
            # Main content area: Data Preview
            st.header("Data Preview")
            col1, col2 = st.columns(2)
            with col1:
                st.dataframe(df.head(), use_container_width=True)
            with col2:
                st.code(display_df_info(df))

            # Sidebar: Model Configuration
            with st.sidebar:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.header("Model Configuration")
                date_col = st.selectbox("Date Column", df.columns)
                cat_cols = st.multiselect("Categorical Columns", df.columns)
                target_col = st.selectbox("Target Column", df.columns)
                model_type = st.selectbox(
                    "Model Type",
                    ["random_forest", "gradient_boosting", "stacking", 
                        "xgboost", "lightgbm", "catboost"]
                )
                
                train_model_btn = st.button("Train Model", type="primary")
                st.markdown('</div>', unsafe_allow_html=True)

            # Main content area: Model Training Results
            if train_model_btn:
                with st.spinner("Training model..."):
                    try:
                        df_processed, log_transformed = preprocess_data(df, date_col, cat_cols, target_col)
                        st.session_state.was_log_transformed = log_transformed
                        
                        model, metrics, X_test, y_test, y_pred = train_model(
                            df_processed, date_col, cat_cols, target_col, model_type
                        )
                        
                        st.session_state.model = model
                        st.session_state.model_config = {
                            'date_col': date_col,
                            'cat_cols': cat_cols,
                            'target_col': target_col,
                            'required_columns': df_processed.columns.tolist()
                        }
                        
                        model_id = st.session_state.model_manager.save_model(
                            model, 
                            metrics, 
                            model_type,
                            test_data=(X_test, y_test, y_pred),
                            config=st.session_state.model_config
                        )
                        
                        st.markdown(apply_style("status",
                            status="success",
                            text="Model trained and saved successfully!"
                        ), unsafe_allow_html=True)
                        
                        st.header("Model Performance")
                        display_all_tabs(model, metrics, X_test, y_test, y_pred, target_col)
                        
                        with st.expander("Best Model Parameters"):
                            st.json(metrics['best_params'])
                        
                    except Exception as e:
                        st.markdown(apply_style("status",
                            status="error",
                            text=f"Error training model: {str(e)}"
                        ), unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
   main()

================
File: components/metrics.py
================
import streamlit as st
from config.styles import apply_style, ColorPalette

def display_detailed_metrics(model, metrics, y_test, y_pred, target_col):
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(apply_style("metric", 
            value=f"{metrics['rmse']:.2f}",
            label="RMSE"
        ), unsafe_allow_html=True)
        
    with col2:
        st.markdown(apply_style("metric",
            value=f"{metrics['mae']:.2f}",
            label="MAE"
        ), unsafe_allow_html=True)
        
    with col3:
        st.markdown(apply_style("metric",
            value=f"{metrics['r2']:.3f}",
            label="RÂ² Score"
        ), unsafe_allow_html=True)
        
    with col4:
        cv_mean = metrics['cv_scores'].mean() if 'cv_scores' in metrics else metrics['cv_score']
        cv_std = metrics['cv_scores'].std() if 'cv_scores' in metrics else 0
        st.markdown(apply_style("metric",
            value=f"{cv_mean:.3f} Â± {cv_std:.3f}",
            label="CV Score"
        ), unsafe_allow_html=True)

================
File: config/styles.py
================
# config/styles.py
from dataclasses import dataclass, field
from typing import Dict

from dataclasses import dataclass, field

@dataclass
class ColorPalette:
    primary: str = "#3b82f6"
    success: str = "#22c55e" 
    warning: str = "#f59e0b"
    error: str = "#ef4444"
    background: str = "#f8fafc"
    text: Dict[str, str] = field(default_factory=lambda: {
        "primary": "#111827",
        "secondary": "#4b5563",
        "light": "#9ca3af"
    })

def get_css():
    return """
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        
        .stApp {
            font-family: 'Inter', sans-serif !important;
            background-color: #f8fafc;
        }
        
        /* Header Styling */
        .app-header {
            background: linear-gradient(to right, #2563eb, #3b82f6);
            color: white;
            padding: 2rem;
            border-radius: 8px;
            margin-bottom: 2rem;
        }
        
        /* Card Components */
        .card {
            background: white;
            padding: 1.5rem;
            border-radius: 0.75rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.12);
            margin-bottom: 1rem;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        /* Metrics Display */
        .metric-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
            gap: 1.25rem;
            margin: 1.5rem 0;
        }
        
        .metric-item {
            background: white;
            padding: 1.5rem;
            border-radius: 0.75rem;
            border-left: 4px solid #3b82f6;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .metric-value {
            font-size: 2rem;
            font-weight: 600;
            color: #111827;
        }
        
        .metric-label {
            font-size: 0.875rem;
            color: #6b7280;
            margin-top: 0.5rem;
        }
        
        /* Status Indicators */
        .status-badge {
            display: inline-flex;
            align-items: center;
            padding: 0.375rem 0.75rem;
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 500;
        }
        
        .status-badge.success { 
            background: #dcfce7;
            color: #166534;
        }
        
        .status-badge.warning {
            background: #fef3c7;
            color: #92400e;
        }
        
        .status-badge.error {
            background: #fee2e2;
            color: #991b1b;
        }
        
        /* Table Improvements */
        .dataframe {
            border: none !important;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        
        .dataframe thead th {
            background-color: #f8fafc !important;
            font-weight: 600 !important;
            padding: 0.75rem 1rem !important;
        }
        
        .dataframe tbody td {
            padding: 0.75rem 1rem !important;
        }
        
        /* Button Styling */
        .stButton > button {
            width: 100%;
            height: 2.75rem;
            border-radius: 6px;
            font-weight: 500;
            transition: all 0.2s;
            background-color: #3b82f6;
            color: white;
        }
        
        .stButton > button:hover {
            background-color: #2563eb;
            transform: translateY(-1px);
        }
        
        /* File Upload Area */
        .uploadedFile {
            border: 2px dashed #e2e8f0;
            border-radius: 8px;
            padding: 1.5rem;
            text-align: center;
            background: #f8fafc;
        }

        /* Tab Navigation */
        .stTabs [data-baseweb="tab-list"] {
            gap: 2rem;
            background-color: transparent;
        }

        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre;
            background-color: transparent;
            border-radius: 4px;
            color: #444;
            font-weight: 500;
        }

        .stTabs [data-baseweb="tab"]:hover {
            background-color: rgba(59, 130, 246, 0.1);
            color: #3b82f6;
        }

        /* Progress Bar */
        .stProgress > div > div {
            background-color: #3b82f6;
        }
    </style>
    """

def apply_style(component_type: str, **kwargs) -> str:
    """Generate HTML with appropriate styling for different components"""
    if component_type == "metric":
        return f"""
        <div class="metric-item">
            <div class="metric-value">{kwargs.get('value', '')}</div>
            <div class="metric-label">{kwargs.get('label', '')}</div>
        </div>
        """
    elif component_type == "status":
        return f"""
        <span class="status-badge {kwargs.get('status', 'success')}">
            {kwargs.get('text', '')}
        </span>
        """
    return ""

================
File: model_management.py
================
import pandas as pd
import streamlit as st
import json
import os
from datetime import datetime
import pickle
import plotly.graph_objects as go

class ModelManager:
    def __init__(self, storage_path='models'):
        self.storage_path = storage_path
        os.makedirs(storage_path, exist_ok=True)
        self.metadata_file = os.path.join(storage_path, 'model_metadata.json')
        self.load_metadata()

    def load_metadata(self):
        try:
            print(f"Storage path: {self.storage_path}")  # Debug
            print(f"Files in storage: {os.listdir(self.storage_path)}")  # Debug
            
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'r') as f:
                    self.metadata = json.load(f)
            else:
                self.metadata = {}
        except Exception as e:
            print(f"Error loading metadata: {str(e)}")
            self.metadata = {}
            if os.path.exists(self.metadata_file):
                os.remove(self.metadata_file)


    def save_model(self, model, metrics, model_type, test_data=None, config=None):
        # Create a more readable timestamp
        current_time = datetime.now()
        readable_timestamp = current_time.strftime('%m-%d-%Y at %H:%M:%S')
        raw_timestamp = current_time.strftime('%Y%m%d_%H%M%S')  # Keep for file naming
        
        model_id = f"{model_type}_{raw_timestamp}"
        model_path = os.path.join(self.storage_path, f"{model_id}.pkl")
        
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)

        test_data_path = None
        if test_data:
            test_data_path = os.path.join(self.storage_path, f"{model_id}_test_data.pkl")
            with open(test_data_path, 'wb') as f:
                pickle.dump(test_data, f)
        
        self.metadata[model_id] = {
            'type': model_type,
            'timestamp': readable_timestamp,  # Store readable format
            'raw_timestamp': raw_timestamp,   # Keep raw format for file operations
            'metrics': {
                'r2': float(metrics['r2']),
                'rmse': float(metrics['rmse']),
                'mae': float(metrics['mae']),
                'cv_score': float(metrics['cv_scores'].mean())
            },
            'path': model_path,
            'test_data_path': test_data_path,
            'config': config
        }
        self.save_metadata()
        return model_id

    def get_model(self, model_id):
        if model_id in self.metadata:
            with open(self.metadata[model_id]['path'], 'rb') as f:
                return pickle.load(f)
        return None

    def get_model_test_data(self, model_id):
        if model_id in self.metadata and self.metadata[model_id].get('test_data_path'):
            with open(self.metadata[model_id]['test_data_path'], 'rb') as f:
                return pickle.load(f)
        return None

    def get_model_config(self, model_id):
        return self.metadata[model_id].get('config', None)

    def delete_model(self, model_id):
        if model_id in self.metadata:
            # Delete model file
            if os.path.exists(self.metadata[model_id]['path']):
                os.remove(self.metadata[model_id]['path'])
            
            # Delete test data if exists
            if self.metadata[model_id].get('test_data_path') and \
               os.path.exists(self.metadata[model_id]['test_data_path']):
                os.remove(self.metadata[model_id]['test_data_path'])
            
            del self.metadata[model_id]
            self.save_metadata()
            return True
        return False

    def display_comparison(self):
        if not self.metadata:
            st.warning("No models saved yet.")
            return

        df = pd.DataFrame.from_dict(
            {k: v['metrics'] for k, v in self.metadata.items()}, 
            orient='index'
        )
        df['model_type'] = [v['type'] for v in self.metadata.values()]
        df['timestamp'] = [v['timestamp'] for v in self.metadata.values()]
        
        st.subheader("Model Comparison")
        
        col1, col2 = st.columns(2)
        with col1:
            st.dataframe(df.round(4))
        
        with col2:
            fig = go.Figure()
            for metric in ['r2', 'rmse', 'mae']:
                fig.add_trace(go.Bar(
                    name=metric.upper(),
                    x=df.index,
                    y=df[metric],
                    text=df[metric].round(3)
                ))
            
            fig.update_layout(
                barmode='group',
                title="Model Metrics Comparison",
                xaxis_title="Model ID",
                yaxis_title="Value"
            )
            st.plotly_chart(fig)
        
        st.subheader("Model Management")
        col1, col2 = st.columns(2)
        
        with col1:
            models_to_delete = st.multiselect(
                "Select models to delete:",
                options=list(self.metadata.keys())
            )
        
        with col2:
            if st.button("Delete Selected Models"):
                for model_id in models_to_delete:
                    if self.delete_model(model_id):
                        st.success(f"Model {model_id} deleted successfully!")
                    else:
                        st.error(f"Failed to delete model {model_id}")
                st.rerun()

================
File: model.py
================
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

import streamlit as st

def calculate_metrics(y_true, y_pred):
   return {
       'mae': mean_absolute_error(y_true, y_pred),
       'mse': mean_squared_error(y_true, y_pred),
       'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
       'r2': r2_score(y_true, y_pred),
       'percent_errors': np.abs((y_true - y_pred) / y_true) * 100
   }


def get_model_config(model_type):
   if model_type == 'random_forest':
       base_model = RandomForestRegressor(random_state=42)
       param_grid = {
           'regressor__n_estimators': [100, 200, 500],
           'regressor__max_depth': [10, 20, 30, None],
           'regressor__min_samples_leaf': [1, 2, 4],
           'regressor__min_samples_split': [2, 5, 10]
       }
   elif model_type == 'gradient_boosting':
       base_model = GradientBoostingRegressor(random_state=42, loss='huber')
       param_grid = {
           'regressor__n_estimators': [100, 200, 500],
           'regressor__learning_rate': [0.01, 0.05, 0.1],
           'regressor__max_depth': [3, 5, 7],
           'regressor__subsample': [0.8, 0.9, 1.0]
       }
   elif model_type == 'stacking':
       estimators = [
           ('rf', RandomForestRegressor(random_state=42)),
           ('gb', GradientBoostingRegressor(random_state=42))
       ]
       base_model = StackingRegressor(
           estimators=estimators,
           final_estimator=RandomForestRegressor(n_estimators=100, random_state=42),
           cv=5
       )
       param_grid = {
           'regressor__final_estimator__n_estimators': [100, 200],
           'regressor__final_estimator__max_depth': [10, 20]
       }
   elif model_type == 'xgboost':
       base_model = XGBRegressor(random_state=42)
       param_grid = {
           'regressor__n_estimators': [500, 1000],
           'regressor__learning_rate': [0.01, 0.05],
           'regressor__max_depth': [4, 6, 8],
           'regressor__subsample': [0.8, 0.9]
       }
   elif model_type == 'lightgbm':
       base_model = LGBMRegressor(random_state=42)
       param_grid = {
           'regressor__n_estimators': [500, 1000],
           'regressor__learning_rate': [0.01, 0.05],
           'regressor__max_depth': [4, 6, 8],
           'regressor__num_leaves': [31, 63]
       }
   elif model_type == 'catboost':
        base_model = CatBoostRegressor(
            iterations=1000,
            learning_rate=0.03,
            depth=6,
            l2_leaf_reg=3,
            min_data_in_leaf=10,
            grow_policy='Depthwise',
            random_strength=1,
            bagging_temperature=1,
            od_type='Iter',
            od_wait=20,
            verbose=False,
            random_state=42,
            allow_writing_files=False,
            task_type='CPU',
            thread_count=-1,
            loss_function='RMSE',
            eval_metric='RMSE',
            bootstrap_type='Bayesian',
            leaf_estimation_method='Newton'
        )
        
        param_grid = {
            'regressor__iterations': [1000],
            'regressor__learning_rate': [0.01, 0.03],
            'regressor__depth': [4, 6],
            'regressor__min_data_in_leaf': [10],
            'regressor__grow_policy': ['Depthwise']
        }

   # Add log transform status for all models
   st.session_state.was_log_transformed = True
   return base_model, param_grid

# In model.py

def create_advanced_features(df, date_col, cat_cols, target_col):
    # Enhanced temporal features
    df['DayOfMonth'] = df[date_col].dt.day
    df['WeekOfMonth'] = df[date_col].dt.day.apply(lambda x: (x-1)//7 + 1)
    df['DayOfYear'] = df[date_col].dt.dayofyear
    
    # Category-based features
    for cat in cat_cols:
        # Target encoding with cross-validation
        df[f'{cat}_mean_spend'] = df.groupby(cat)[target_col].transform('mean')
        df[f'{cat}_median_spend'] = df.groupby(cat)[target_col].transform('median')
        
        # Time-based category features
        df[f'{cat}_month_spend'] = df.groupby([cat, 'Month'])[target_col].transform('mean')
        df[f'{cat}_quarter_spend'] = df.groupby([cat, 'Quarter'])[target_col].transform('mean')
        
        # Frequency features
        df[f'{cat}_count'] = df.groupby(cat)[target_col].transform('count')
        
    return df


def add_features(df, date_col, cat_cols, target_col):
    numeric_features = ['Year', 'Month', 'DayOfWeek', 'Quarter', 'WeekOfYear',
                       'IsWeekend', 'Season', 'IsHoliday']
    numeric_features.extend([col for col in df.columns if 'lag_' in col or 'rolling_' in col])
    
    for cat in cat_cols:
        df[f'{cat}_mean_spend'] = df.groupby(cat)[target_col].transform('mean')
        df[f'{cat}_median_spend'] = df.groupby(cat)[target_col].transform('median')
        df[f'{cat}_month_spend'] = df.groupby([cat, 'Month'])[target_col].transform('mean')
        df[f'{cat}_quarter_spend'] = df.groupby([cat, 'Quarter'])[target_col].transform('mean')
        df[f'{cat}_count'] = df.groupby(cat)[target_col].transform('count')
        numeric_features.extend([f'{cat}_mean_spend', f'{cat}_median_spend',
                              f'{cat}_month_spend', f'{cat}_quarter_spend', f'{cat}_count'])
    
    return df, numeric_features

def train_model(df, date_col, cat_cols, target_col, model_type='random_forest'):
    df, numeric_features = add_features(df, date_col, cat_cols, target_col)
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', RobustScaler(), numeric_features),
            ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)
        ]
    )
    
    base_model, param_grid = get_model_config(model_type)
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', base_model)
    ])
    
    X = df[[date_col] + cat_cols + numeric_features]
    y = df[target_col]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    grid_search = GridSearchCV(
        pipeline,
        param_grid,
        cv=5,
        scoring=['r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'],
        refit='r2',
        n_jobs=-1
    )
    grid_search.fit(X_train, y_train)
    
    y_pred = grid_search.predict(X_test)
    
    metrics = {
        'mae': mean_absolute_error(y_test, y_pred),
        'mse': mean_squared_error(y_test, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
        'r2': r2_score(y_test, y_pred),
        'cv_scores': cross_val_score(grid_search.best_estimator_, X, y, cv=5, scoring='r2'),
        'percent_errors': np.abs((y_test - y_pred) / y_test) * 100,
        'best_params': grid_search.best_params_,
        'feature_names': numeric_features + cat_cols
    }
    
    return grid_search.best_estimator_, metrics, X_test, y_test, y_pred

================
File: models/model_metadata.json
================
{
  "catboost_20241203_204640": {
    "type": "catboost",
    "timestamp": "20241203_204640",
    "metrics": {
      "r2": 0.4472059663158162,
      "rmse": 1.1976271575649813,
      "mae": 0.9506827380753325,
      "cv_score": 0.44885828567830793
    },
    "path": "models\\catboost_20241203_204640.pkl",
    "test_data_path": "models\\catboost_20241203_204640_test_data.pkl",
    "config": {
      "date_col": "Date",
      "cat_cols": [
        "FinalCategory"
      ],
      "target_col": "Spend",
      "required_columns": [
        "Date",
        "FinalCategory",
        "Spend",
        "Year",
        "Month",
        "DayOfWeek",
        "Quarter",
        "WeekOfYear",
        "IsWeekend",
        "Season",
        "IsHoliday",
        "lag_1_FinalCategory",
        "lag_7_FinalCategory",
        "lag_30_FinalCategory",
        "rolling_mean_7_FinalCategory",
        "rolling_std_7_FinalCategory",
        "rolling_mean_30_FinalCategory",
        "rolling_std_30_FinalCategory",
        "FinalCategory_mean_spend",
        "FinalCategory_median_spend",
        "FinalCategory_month_spend",
        "FinalCategory_quarter_spend",
        "FinalCategory_count"
      ]
    }
  },
  "catboost_20241204_103012": {
    "type": "catboost",
    "timestamp": "20241204_103012",
    "metrics": {
      "r2": 0.4472059663158162,
      "rmse": 1.1976271575649813,
      "mae": 0.9506827380753325,
      "cv_score": 0.44885828567830793
    },
    "path": "models\\catboost_20241204_103012.pkl",
    "test_data_path": "models\\catboost_20241204_103012_test_data.pkl",
    "config": {
      "date_col": "Date",
      "cat_cols": [
        "FinalCategory"
      ],
      "target_col": "Spend",
      "required_columns": [
        "Date",
        "FinalCategory",
        "Spend",
        "Year",
        "Month",
        "DayOfWeek",
        "Quarter",
        "WeekOfYear",
        "IsWeekend",
        "Season",
        "IsHoliday",
        "lag_1_FinalCategory",
        "lag_7_FinalCategory",
        "lag_30_FinalCategory",
        "rolling_mean_7_FinalCategory",
        "rolling_std_7_FinalCategory",
        "rolling_mean_30_FinalCategory",
        "rolling_std_30_FinalCategory",
        "FinalCategory_mean_spend",
        "FinalCategory_median_spend",
        "FinalCategory_month_spend",
        "FinalCategory_quarter_spend",
        "FinalCategory_count"
      ]
    }
  },
  "random_forest_20241204_104140": {
    "type": "random_forest",
    "timestamp": "20241204_104140",
    "metrics": {
      "r2": 0.4413030234831442,
      "rmse": 1.2040045346281638,
      "mae": 0.963603258437521,
      "cv_score": 0.45794531209320344
    },
    "path": "models\\random_forest_20241204_104140.pkl",
    "test_data_path": "models\\random_forest_20241204_104140_test_data.pkl",
    "config": {
      "date_col": "Date",
      "cat_cols": [
        "FinalCategory"
      ],
      "target_col": "Spend",
      "required_columns": [
        "Date",
        "FinalCategory",
        "Spend",
        "Year",
        "Month",
        "DayOfWeek",
        "Quarter",
        "WeekOfYear",
        "IsWeekend",
        "Season",
        "IsHoliday",
        "lag_1_FinalCategory",
        "lag_7_FinalCategory",
        "lag_30_FinalCategory",
        "rolling_mean_7_FinalCategory",
        "rolling_std_7_FinalCategory",
        "rolling_mean_30_FinalCategory",
        "rolling_std_30_FinalCategory",
        "FinalCategory_mean_spend",
        "FinalCategory_median_spend",
        "FinalCategory_month_spend",
        "FinalCategory_quarter_spend",
        "FinalCategory_count"
      ]
    }
  }
}

================
File: preprocessing.py
================
import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
from datetime import datetime

def display_df_info(df):
   info = []
   info.append(f"Total Rows: {len(df)}")
   info.append(f"Total Columns: {len(df.columns)}")
   info.append("\nColumn Info:")
   for col in df.columns:
       dtype = str(df[col].dtype)
       nulls = df[col].isnull().sum()
       info.append(f"{col}: {dtype} | Null Values: {nulls}")
   return "\n".join(info)

def create_date_features(df, date_col):
    df[date_col] = pd.to_datetime(df[date_col])
    
    try:
        df['Year'] = df[date_col].dt.year
        df['Month'] = df[date_col].dt.month 
        df['DayOfWeek'] = df[date_col].dt.dayofweek
        df['Quarter'] = df[date_col].dt.quarter
        df['WeekOfYear'] = df[date_col].dt.isocalendar().week
        df['IsWeekend'] = df[date_col].dt.dayofweek.isin([5, 6]).astype(int)
        df['Season'] = df[date_col].dt.month % 12 // 3 + 1
        
        holidays = pd.DataFrame({
            'date': pd.date_range(start=df[date_col].min(), end=df[date_col].max()),
            'is_holiday': 0
        })
        
        df['IsHoliday'] = df[date_col].map(
            holidays.set_index('date')['is_holiday']).fillna(0)
            
        return df
    except Exception as e:
        raise Exception(f"Error creating date features: {str(e)}")

def create_lag_features(df, date_col, target_col, cat_cols):
   """Create lagged and rolling features"""
   df = df.sort_values(date_col)
   
   for cat in cat_cols:
       # Lag features
       for lag in [1, 7, 30]:
           df[f'lag_{lag}_{cat}'] = df.groupby(cat)[target_col].shift(lag)
       
       # Rolling statistics
       for window in [7, 30]:
           df[f'rolling_mean_{window}_{cat}'] = df.groupby(cat)[target_col].transform(
               lambda x: x.rolling(window=window, min_periods=1).mean())
           df[f'rolling_std_{window}_{cat}'] = df.groupby(cat)[target_col].transform(
               lambda x: x.rolling(window=window, min_periods=1).std())
   
   return df

def handle_outliers(df, target_col):
   """Remove outliers using RobustScaler"""
   scaler = RobustScaler()
   scaled_target = scaler.fit_transform(df[[target_col]])
   return df[abs(scaled_target.ravel()) < 3]

def preprocess_data(df, date_col, cat_cols, target_col):
   """Main preprocessing pipeline"""
   df = df.copy()
   
   # Handle outliers
   df = handle_outliers(df, target_col)
   
   # Create features
   df = create_date_features(df, date_col)
   df = create_lag_features(df, date_col, target_col, cat_cols)
   
   # Fill missing values
   df = df.ffill().bfill()
   
   # Log transform if skewed
   was_log_transformed = False
   if df[target_col].skew() > 1:
       df[target_col] = np.log1p(df[target_col])
       was_log_transformed = True
   
   return df, was_log_transformed

================
File: requirements.txt
================
streamlit
streamlit-card
pandas
scikit-learn
plotly
matplotlib
xgboost
lightgbm
catboost
pmdarima
fbprophet
